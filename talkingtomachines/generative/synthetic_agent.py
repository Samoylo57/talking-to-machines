from typing import Any, List, Callable
from talkingtomachines.generative.prompt import (
    generate_conversational_agent_system_message,
    generate_profile_prompt,
)
from talkingtomachines.generative.llm import query_llm
from openai import OpenAI
from talkingtomachines.config import DevelopmentConfig

ProfileInfo = dict[str, Any]


class SyntheticAgent:
    """A class for constructing the base synthetic agent.

    Args:
        experiment_id (str): The ID of the experiment.
        experiment_context (str): The context of the experiment.
        session_id (int): The ID of the session.
        profile_info (ProfileInfo): The profile information of the user.
        model_info (str): The information about the model used by the agent.
        api_endpoint (str, optional): API endpoint to the LLM model if the model is hosted externally.
        profile_prompt_generator (Callable[[ProfileInfo], str], optional):
            A function that generates a profile prompt based on the profile information.
            Defaults to generate_profile_prompt.

    Attributes:
        experiment_id (str): The ID of the experiment.
        experiment_context (str): The context of the experiment.
        session_id (int): The ID of the session.
        profile_info (ProfileInfo): The profile information of the user.
        profile_prompt (str): A prompt string containing the profile information of the user.
        model_info (str): The information about the model used by the agent.
        llm_client (OpenAI): The LLM client.
    """

    def __init__(
        self,
        experiment_id: str,
        experiment_context: str,
        session_id: int,
        profile_info: ProfileInfo,
        model_info: str,
        api_endpoint: str,
        profile_prompt_generator: Callable[
            [ProfileInfo], str
        ] = generate_profile_prompt,
    ):
        self.experiment_id = experiment_id
        self.experiment_context = experiment_context
        self.session_id = session_id
        self.profile_info = profile_info
        self.profile_prompt = profile_prompt_generator(profile_info)
        self.model_info = model_info
        self.llm_client = self.initialise_llm_client(self.model_info, api_endpoint)

    def initialise_llm_client(self, model_info: str, api_endpoint: str = ""):
        """Initialise a language model client based on the provided model information.

        Args:
            model_info (str): The identifier for the language model to be used.
                      Supported values are "gpt-4o", "gpt-4o-mini", "gpt-4-turbo",
                      "gpt-4", "gpt-3.5-turbo", and "hf-inference".
            api_endpoint (str, optional): The API endpoint to be used for the "hf-inference" model.
                          Defaults to an empty string.

        Returns:
            OpenAI: An instance of the OpenAI client configured with the appropriate API key
                and endpoint based on the model information.

        Raises:
            ValueError: If the provided model_info is not supported.
        """
        if model_info in [
            "gpt-4o",
            "gpt-4o-mini",
            "gpt-4-turbo",
            "gpt-4",
            "gpt-3.5-turbo",
        ]:
            return OpenAI(api_key=DevelopmentConfig.OPENAI_API_KEY)

        elif model_info in ["hf-inference"]:
            return OpenAI(base_url=api_endpoint, api_key=DevelopmentConfig.HF_TOKEN)

        else:
            raise ValueError(f"{model_info} is not supported.")

    def to_dict(self) -> dict[str, Any]:
        """Converts the SyntheticAgent object to a dictionary.

        Returns:
            dict[str, Any]: A dictionary representation of the SyntheticAgent object.
        """
        return {
            "experiment_id": self.experiment_id,
            "experiment_context": self.experiment_context,
            "session_id": self.session_id,
            "profile_info": self.profile_info,
            "profile_prompt": self.profile_prompt,
            "model_info": self.model_info,
        }

    def respond(self) -> str:
        """Generate a response based on the synthetic agent's model.

        Returns:
            str: The response generated by the synthetic agent
        """
        try:
            return ""
        except Exception as e:
            # Log the exception
            print(f"Error during response generation in SyntheticAgent object: {e}")
            return None


class ConversationalSyntheticAgent(SyntheticAgent):
    """A synthetic agent that interacts with users in a conversational system. Inherits from the SyntheticAgent base class.

    Args:
        experiment_id (str): The ID of the experiment.
        experiment_context (str): The context of the experiment.
        session_id (int): The ID of the session.
        profile_info (ProfileInfo): The profile information of the user.
        role (str): The name of the role assigned to the agent.
        role_description (str): The description of the role assigned to the agent.
        model_info (str): The information about the model used by the agent.
        api_endpoint (str, optional): API endpoint to the LLM model if the model is hosted externally.
        treatment (str): The treatment assigned to the session.
        profile_prompt_generator (Callable[[ProfileInfo], str], optional):
            A function that generates a profile prompt based on the profile information.
            Defaults to generate_profile_prompt.

    Attributes:
        experiment_id (str): The ID of the experiment.
        experiment_context (str): The context of the experiment.
        session_id (int): The ID of the session.
        profile_info (ProfileInfo): The profile information of the user.
        profile_prompt (str): A prompt string containing the profile information of the user.
        model_info (str): The information about the model used by the agent.
        llm_client (OpenAI): The LLM client.
        role (str): The name of the role assigned to the agent.
        role_description (str): The description of the role assigned to the agent.
        treatment (str): The treatment assigned to the session.
        system_message (str): The system message generated for the conversation.
        message_history (List[dict]): The history of the conversation with the synthetic agent.
    """

    def __init__(
        self,
        experiment_id: str,
        experiment_context: str,
        session_id: int,
        profile_info: ProfileInfo,
        role: str,
        role_description: str,
        model_info: str,
        api_endpoint: str,
        treatment: str,
        profile_prompt_generator: Callable[
            [ProfileInfo], str
        ] = generate_profile_prompt,
    ):
        super().__init__(
            experiment_id,
            experiment_context,
            session_id,
            profile_info,
            model_info,
            api_endpoint,
            profile_prompt_generator,
        )
        self.role = role
        self.role_description = role_description
        self.treatment = treatment
        self.system_message = generate_conversational_agent_system_message(
            experiment_context=self.experiment_context,
            treatment=self.treatment,
            role_description=self.role_description,
            profile_prompt=self.profile_prompt,
        )
        self.message_history = [
            {"role": "system", "content": self.system_message},
        ]

    def to_dict(self) -> dict[str, Any]:
        """Converts the ConversationalSyntheticAgent object to a dictionary.

        Returns:
            dict[str, Any]: A dictionary representation of the ConversationalSyntheticAgent object.
        """
        return {
            "experiment_id": self.experiment_id,
            "experiment_context": self.experiment_context,
            "session_id": self.session_id,
            "profile_info": self.profile_info,
            "profile_prompt": self.profile_prompt,
            "model_info": self.model_info,
            "role": self.role,
            "role_description": self.role_description,
            "treatment": self.treatment,
            "system_message": self.system_message,
            "message_history": self.message_history,
        }

    def update_message_history(self, message: str, role: str) -> None:
        """Update the message history of the synthetic agent with a new message.

        Args:
            message (str): A message to add to the conversation history.
            role (str): The identifier of the party that generated the message.

        Returns:
            None
        """
        self.message_history.append({"role": role, "content": message})

    def respond(self, question: str) -> str:
        """Generate a response to a question posed to the synthetic agent.

        Args:
            question (str): A question or prompt to which the agent should respond.

        Returns:
            str: The response generated by the synthetic agent.
        """
        try:
            self.update_message_history(message=question, role="assistant")
            response = query_llm(
                llm_client=self.llm_client,
                model_info=self.model_info,
                message_history=self.message_history,
            )
            self.update_message_history(message=response, role="user")
            return response

        except Exception as e:
            # Log the exception
            print(
                f"Error during response generation by ConversationalSyntheticAgent object: {e}"
            )
            return ""
