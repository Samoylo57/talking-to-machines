import re
from typing import Any, Callable
from talkingtomachines.generative.prompt import (
    generate_conversational_agent_system_message,
    generate_profile_prompt,
)
from talkingtomachines.generative.llm import query_llm
from openai import OpenAI
from talkingtomachines.config import DevelopmentConfig

ProfileInfo = dict[str, Any]
NUM_RETRY = 3
OPENAI_MODELS = [
    "gpt-4.5-preview",
    "o3",
    "o4-mini",
    "o1-pro",
    "o1",
    "gpt-4.1",
    "gpt-4.1-mini",
    "gpt-4.1-nano",
    "gpt-4o",
    "gpt-4o-mini",
    "gpt-4-turbo",
    "gpt-4",
    "gpt-3.5-turbo",
]


class SyntheticAgent:
    """A class for constructing the base synthetic agent.

    Args:
        experiment_id (str): The ID of the experiment.
        experiment_context (str): The context of the experiment.
        session_id (Any): The ID of the session.
        profile_info (ProfileInfo): The profile information of the user.
        model_info (str): The information about the model used by the agent.
        api_endpoint (str, optional): API endpoint to the LLM model if the model is hosted externally.
        profile_prompt_generator (Callable[[ProfileInfo], str], optional):
            A function that generates a profile prompt based on the profile information.
            Defaults to generate_profile_prompt.

    Attributes:
        experiment_id (str): The ID of the experiment.
        experiment_context (str): The context of the experiment.
        session_id (Any): The ID of the session.
        profile_info (ProfileInfo): The profile information of the user.
        profile_prompt (str): A prompt string containing the profile information of the user.
        model_info (str): The information about the model used by the agent.
        api_endpoint (str): API endpoint to the LLM model if the model is hosted externally.
        llm_client (OpenAI): The LLM client.
    """

    def __init__(
        self,
        experiment_id: str,
        experiment_context: str,
        session_id: Any,
        profile_info: ProfileInfo,
        model_info: str,
        api_endpoint: str = "",
        profile_prompt_generator: Callable[
            [ProfileInfo], str
        ] = generate_profile_prompt,
    ):
        self.experiment_id = experiment_id
        self.experiment_context = experiment_context
        self.session_id = session_id
        self.profile_info = profile_info
        self.profile_prompt = profile_prompt_generator(profile_info)
        self.model_info = model_info
        self.api_endpoint = api_endpoint
        self.llm_client = self.initialise_llm_client()

    def initialise_llm_client(self):
        """Initialise a language model client based on the provided model information and API endpoint.

        Returns:
            OpenAI: An instance of the OpenAI client configured with the appropriate API key
                and endpoint based on the model information.

        Raises:
            ValueError: If the provided model_info is not supported.
        """
        if self.model_info in OPENAI_MODELS:
            return OpenAI(api_key=DevelopmentConfig.OPENAI_API_KEY)

        elif self.model_info in ["hf-inference"]:
            return OpenAI(
                base_url=self.api_endpoint, api_key=DevelopmentConfig.HF_TOKEN
            )

        else:
            raise ValueError(f"{self.model_info} is not supported.")

    def to_dict(self) -> dict[str, Any]:
        """Converts the SyntheticAgent object to a dictionary.

        Returns:
            dict[str, Any]: A dictionary representation of the SyntheticAgent object.
        """
        return {
            "experiment_id": self.experiment_id,
            "experiment_context": self.experiment_context,
            "session_id": self.session_id,
            "profile_info": self.profile_info,
            "profile_prompt": self.profile_prompt,
            "model_info": self.model_info,
            "api_endpoint": self.api_endpoint,
        }

    def respond(self) -> str:
        """Generate a response based on the synthetic agent's model.

        Returns:
            str: The response generated by the synthetic agent
        """
        try:
            return ""
        except Exception as e:
            # Log the exception
            print(f"Error during response generation in SyntheticAgent object: {e}")
            return None


class ConversationalSyntheticAgent(SyntheticAgent):
    """A synthetic agent that interacts with users in a conversational system. Inherits from the SyntheticAgent base class.

    Args:
        experiment_id (str): The ID of the experiment.
        experiment_context (str): The context of the experiment.
        session_id (Any): The ID of the session.
        profile_info (ProfileInfo): The profile information of the user.
        model_info (str): The information about the model used by the agent.
        api_endpoint (str, optional): API endpoint to the LLM model if the model is hosted externally.
        role (str): The name of the role assigned to the agent.
        role_description (str): The description of the role assigned to the agent.
        treatment (str): The treatment assigned to the session.
        profile_prompt_generator (Callable[[ProfileInfo], str], optional):
            A function that generates a profile prompt based on the profile information.
            Defaults to generate_profile_prompt.

    Attributes:
        experiment_id (str): The ID of the experiment.
        experiment_context (str): The context of the experiment.
        session_id (Any): The ID of the session.
        profile_info (ProfileInfo): The profile information of the user.
        profile_prompt (str): A prompt string containing the profile information of the user.
        model_info (str): The information about the model used by the agent.
        api_endpoint (str): API endpoint to the LLM model if the model is hosted externally.
        role (str): The name of the role assigned to the agent.
        role_description (str): The description of the role assigned to the agent.
        treatment (str): The treatment assigned to the session.
        system_message (str): The system message generated for the conversation.
        llm_client (OpenAI): The LLM client.
        message_history (List[dict]): The history of the conversation with the synthetic agent.
    """

    def __init__(
        self,
        experiment_id: str,
        experiment_context: str,
        session_id: Any,
        profile_info: ProfileInfo,
        model_info: str,
        api_endpoint: str,
        role: str,
        role_description: str,
        treatment: str,
        profile_prompt_generator: Callable[
            [ProfileInfo], str
        ] = generate_profile_prompt,
    ):
        super().__init__(
            experiment_id,
            experiment_context,
            session_id,
            profile_info,
            model_info,
            api_endpoint,
            profile_prompt_generator,
        )
        self.role = role
        self.role_description = role_description
        self.treatment = treatment
        self.system_message = generate_conversational_agent_system_message(
            experiment_context=self.experiment_context,
            treatment=self.treatment,
            role_description=self.role_description,
            profile_prompt=self.profile_prompt,
        )
        self.message_history = [
            {"role": "system", "content": self.system_message},
        ]

    def to_dict(self) -> dict[str, Any]:
        """Converts the ConversationalSyntheticAgent object to a dictionary.

        Returns:
            dict[str, Any]: A dictionary representation of the ConversationalSyntheticAgent object.
        """
        return {
            "experiment_id": self.experiment_id,
            "experiment_context": self.experiment_context,
            "session_id": self.session_id,
            "profile_info": self.profile_info,
            "profile_prompt": self.profile_prompt,
            "model_info": self.model_info,
            "api_endpoint": self.api_endpoint,
            "role": self.role,
            "role_description": self.role_description,
            "treatment": self.treatment,
            "system_message": self.system_message,
            "message_history": self.message_history,
        }

    def update_message_history(self, message: str, role: str) -> None:
        """Update the message history of the synthetic agent with a new message.

        Args:
            message (str): A message to add to the conversation history.
            role (str): The identifier of the party that generated the message.

        Returns:
            None
        """
        self.message_history.append({"role": role, "content": message})

    def validate_response(self, response: str, response_options: Any) -> bool:
        """Validates whether a given response contains any of the valid response options as whole words.

        Args:
            response (str): The response string to validate.
            response_options (Any): The valid response options. Can be a range, a list, or a single value.

        Returns:
            bool: True if the response contains any of the valid options as whole words, False otherwise.
        """
        # Build a list of valid options as strings.
        if isinstance(response_options, range):
            options = [str(opt) for opt in response_options]
        elif isinstance(response_options, list):
            options = [str(opt) for opt in response_options]
        else:
            options = [str(response_options)]

        # Check if any valid option appears as a whole word inside the response.
        for opt in options:
            pattern = r"\b" + re.escape(opt) + r"\b"
            if re.search(pattern, response):
                return True
        return False

    def respond(
        self,
        question: str,
        validate_response: str = "0",
        response_options: Any = [],
        generate_speculation_score: str = "0",
    ) -> str:
        """Generate a response to a question posed to the synthetic agent.

        Args:
            question (str): A question or prompt to which the agent should respond.
            validate_response (str): Either "0" or "1" to indicate whether to perform response validation or not
            response_options (Any): A list of options to choose from for the response.
            generate_speculation_score (str): Either "0" or "1" to indicate whether to generate a speculation score or not.

        Returns:
            str: The response generated by the synthetic agent.
        """
        try:
            if generate_speculation_score == "1":
                speculation_instruction = "\n\nAt the end of your response, please include a speculation score from 0 (not speculative at all) to 100 (fully speculative) in the format:\nSpeculation Score: XXX"
                question += speculation_instruction
            self.update_message_history(message=question, role="user")

            if validate_response == "1":  # Validate response based on response_options
                for _ in range(NUM_RETRY):
                    response = query_llm(
                        llm_client=self.llm_client,
                        model_info=self.model_info,
                        message_history=self.message_history,
                    )
                    if validate_response(
                        response=response, response_options=response_options
                    ):
                        break

            else:  # Skip validation
                response = query_llm(
                    llm_client=self.llm_client,
                    model_info=self.model_info,
                    message_history=self.message_history,
                )

            self.update_message_history(message=response, role="assistant")
            return response

        except Exception as e:
            # Log the exception
            print(
                f"Error during response generation by ConversationalSyntheticAgent object: {e}"
            )
            return ""
